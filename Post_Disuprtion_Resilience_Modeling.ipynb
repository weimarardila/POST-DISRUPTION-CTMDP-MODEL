{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1E_WqaBpKDr"
      },
      "source": [
        "#**POST-DISRUPTION MODELING FOR RESILIENCE IMPROVEMENT** \n",
        "\n",
        "Continuous-Time Markov Decision Process (CTMDP) model for improving post-disruption resilience (system's recovery). \n",
        "\n",
        "0. Libraries\n",
        "1. Input files (generator matrices)\n",
        "2. Model functions\n",
        "3. Value iteration\n",
        "4. Policy simulation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (0) Libraries"
      ],
      "metadata": {
        "id": "-OQ_xLUX42Oy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utHt3dywfpCp"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as sps\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sRIexnitkz3"
      },
      "source": [
        "# (1) Importing input files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7PdSrwff8Ih"
      },
      "source": [
        "file = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (2) Model functions"
      ],
      "metadata": {
        "id": "ns3P2MmG5Z0f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIbBNWlpeDhr"
      },
      "source": [
        "## Uniformization model function\n",
        "\n",
        "Description: The uniformization algorithm is a technique to retrieve the probability transition matrices from the generator (rates) matrices.\n",
        "\n",
        "Inputs: Generator matrix (Q)\n",
        "\n",
        "$P = I + (1/\\gamma)*Q$\n",
        "\n",
        "Output: Transition/Stochastic matrix (P)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsXRLawtaPfu"
      },
      "source": [
        "#Uniformization function\n",
        "def prob (rate_matrix):\n",
        "  Q = rate_matrix\n",
        "  I = np.identity(len(Q))\n",
        "  gamma = max(abs(np.diagonal(Q)))\n",
        "  P = I + (1/gamma)*Q\n",
        "  return P"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gV-uRRfOGde"
      },
      "source": [
        "#gamma function\n",
        "def gamma (rate_matrix):\n",
        "  gamma = max(abs(np.diagonal(rate_matrix)))\n",
        "  return gamma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3kGSdAVEtpn"
      },
      "source": [
        "# Reading all transition matrices\n",
        "m = 5\n",
        "Probabilities_matrix = []\n",
        "Rates_matrix = []\n",
        "gamma_array = []\n",
        "for i in range(m):\n",
        "  name = 'rates_matrix_action_{}.csv'\n",
        "  instance = pd.read_csv(name.format(i), header=None)\n",
        "  Probabilities_matrix.append(prob(instance))\n",
        "  Rates_matrix.append(instance)\n",
        "  gamma_array.append(gamma(instance)) # Create gammas array for discounted model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv9Y2_TwUnp5"
      },
      "source": [
        "## Actions set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XjtP1z6ckxF"
      },
      "source": [
        "#Actions dictionary\n",
        "action_set =('ss', 'sm', 'mm', 'ml', 'll') # actions names\n",
        "a_n = (0,1,2,3,4) # actions numbers a= {0,1,2,3,4}\n",
        "action_dict ={}\n",
        "\n",
        "#Populating atcion dictionary\n",
        "for i in a_n:\n",
        "  action_dict[a_n[i]] = action_set[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDjDMllSxZSI"
      },
      "source": [
        "## Costs function, $r(s,a)$\n",
        "$r(s,a) = k(s,a) + c(s,a)*E[\\Delta t]$\n",
        "\n",
        "$E[∆t] = \\frac{|S|}{\\sum(mu_{ij})}$ for each $i \\in S $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDXvdUj4xfhY"
      },
      "source": [
        "#k(s,a) |S| x |a|\n",
        "fixed_cost_matrix = pd.read_csv('costs.csv', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1IT992BXkfq"
      },
      "source": [
        "#Expected transition sojurn time\n",
        "expected_times_matrix = pd.read_csv('expected_times.csv', header=None)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL7ebYMZf0c2"
      },
      "source": [
        "#Recovery cost rates\n",
        "cost_rate = pd.read_csv('costs_rates.csv', header = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM1PL2VP9kaK"
      },
      "source": [
        "c_matrix = [np.array(fixed_cost_matrix[0]+cost_rate[0]*expected_times_matrix[0]), np.array(fixed_cost_matrix[1]+cost_rate[0]*expected_times_matrix[1]), np.array(fixed_cost_matrix[2]+cost_rate[0]*expected_times_matrix[2]), np.array(fixed_cost_matrix[3]+cost_rate[0]*expected_times_matrix[3]), np.array(fixed_cost_matrix[4]+cost_rate[0]*expected_times_matrix[4])]\n",
        "\n",
        "#Expected recovery costs, r(s,a)\n",
        "cost_matrix = pd.DataFrame(np.transpose(c_matrix))\n",
        "cost_matrix.replace([np.inf, -np.inf], 0, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3krDJqUcL0"
      },
      "source": [
        "## Data strcuture and preparation\n",
        "### Tupple -> (trans probs, next state, costs)\n",
        "trans prob -> (action, next state, current state)\n",
        "\n",
        "next state -> (i->j)\n",
        "\n",
        "costs/rewards -> (action, state)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxZSiuzZGiwb"
      },
      "source": [
        "p = Probabilities_matrix # p -> list of transition probabilities matrices\n",
        "c = cost_matrix # c -> cost matrix\n",
        "\n",
        "# Creating lists\n",
        "list1= []\n",
        "list2= []\n",
        "for s in range(len(p[0])): \n",
        "  array_2 = []\n",
        "  for a in range(len(a_n)):\n",
        "    array_1 = []\n",
        "    for j in range(len(p[0])):    \n",
        "      array_1.append((p[a][j][s],j,c[a][s]))\n",
        "    array_2.append(array_1)\n",
        "    list1.append(array_1)\n",
        "  list2.append(array_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esN7Ut1H_sof"
      },
      "source": [
        "#Population directory\n",
        "cs =[]\n",
        "for i in range(60):\n",
        "  cs.append(i)\n",
        "  \n",
        "states_dir = {}\n",
        "for i in range(len(p[0])):\n",
        "  states_dir[cs[i]] = list2[i]    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA16bs4hl1CO"
      },
      "source": [
        "# (3) Solution method: Value iteration algorithm\n",
        "\n",
        "Optimization objective: Costs minimization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Value iteration function"
      ],
      "metadata": {
        "id": "Me0JX6Nu7XHE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5511O2AJl4sS"
      },
      "source": [
        "# value iteration function\n",
        "# inputs for value function -> number of states, number of actions, tupple\n",
        "def value_iteration (n_states, n_actions, P, alpha =.00005):\n",
        "  value_table = np.zeros(n_states)\n",
        "  no_of_iterations = 100000\n",
        "  epsilon = 1e-50\n",
        "  for i in range(no_of_iterations):\n",
        "    updated_value_table = np.copy(value_table)\n",
        "    for state in range(n_states):\n",
        "      Q_value = []\n",
        "      for action in range(0, n_actions):\n",
        "        next_states_rewards = []\n",
        "        for next_sr in P[state][action]:\n",
        "          trans_prob, next_state, reward_prob = next_sr\n",
        "          next_states_rewards.append((trans_prob*(reward_prob + (gamma_array[action]/(gamma_array[action]+alpha)) * updated_value_table[next_state])))\n",
        "        Q_value.append(np.sum(next_states_rewards))\n",
        "      value_table[state] = min(Q_value)\n",
        "    if (np.sum(np.fabs(updated_value_table-value_table)) <= epsilon) :\n",
        "      print ('Value-iteration converged at iteration # %d.' %(i+1))\n",
        "      break\n",
        "    #print ('iteration # %d.' %(i+1))\n",
        "  return value_table\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIAM8ZU2WvqY"
      },
      "source": [
        "Now, we define a function called extract policy for extracting optimal policy from the optimal value function. i.e We calculate Q value using our optimal value function and pick up the actions which has the highest Q value for each state as the optimal policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QsTfOi8Qme8"
      },
      "source": [
        "def extract_policy(value_table, n_action, P, alpha =.00005):\n",
        "  # initialize the policy with zeros\n",
        "  policy = np.zeros(len(value_table))   \n",
        "  for state in range(len(value_table)):\n",
        "  # initialize the Q table for a state\n",
        "    Q_table = np.zeros(n_action) \n",
        "    # compute Q value for all ations in the state\n",
        "    for action in range(n_action):\n",
        "      for next_sr in P[state][action]: \n",
        "        trans_prob, next_state, reward_prob = next_sr \n",
        "        Q_table[action] += (trans_prob * (reward_prob + (gamma_array[action]/(gamma_array[action]+alpha)) * value_table[next_state]))\n",
        "    # select the action which has maximum Q value as an optimal action of the state\n",
        "    policy[state] = np.argmin(Q_table)  \n",
        "  return policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0ZsZUssW7dQ"
      },
      "source": [
        "## Step 1: Compute optimal value function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxS08lj4XEdL",
        "outputId": "aec1f3e1-10a8-4167-a1f9-b744f54e2032"
      },
      "source": [
        "number_actions = len(a_n) # number of actions\n",
        "number_states = len(p[0]) # number of states\n",
        "optimal_value_function = value_iteration(number_states, number_actions, states_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value-iteration converged at iteration # 2078.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9uyBteLYJ2s"
      },
      "source": [
        "## Step 2: Derive optimal policy from optimal value function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1MPYd29YRl2"
      },
      "source": [
        "optimal_policy = extract_policy(optimal_value_function, number_actions, states_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYJ3pMQ5YrPI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7d996c6-de0f-405f-f3c6-84362d6de720"
      },
      "source": [
        "#print( \"Optimal value function ->\", optimal_value_function)\n",
        "print( \"Optimal policy ->\", optimal_policy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal policy -> [1. 1. 1. 1. 1. 1. 2. 2. 2. 3. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 1. 1. 1. 1.\n",
            " 2. 2. 3. 3. 3. 3. 1. 1. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
            " 3. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k1K_x2AZVuA"
      },
      "source": [
        "# Optimal policy simulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Eo-TXm2GjkE"
      },
      "source": [
        "#Uniform transition rates\n",
        "m = 5\n",
        "exp_rates = [] #exponential rates after removing \\inf\n",
        "\n",
        "for i in range(m):\n",
        "  name = 'time_rates_matrix_{}.csv'\n",
        "  r = pd.read_csv(name.format(i), header=None)\n",
        "  exp_rates.append(r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLZsyqaGFcVT"
      },
      "source": [
        "def state_generator(rate, i):\n",
        "  rv_vector = []\n",
        "  for j in range(len(rate)):\n",
        "    rv_vector.append(np.random.exponential(rate[j].loc[i]))\n",
        "  time = min(i for i in rv_vector if i > 0)\n",
        "  return rv_vector.index(time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJZfqCywm9k8"
      },
      "source": [
        "# (4) Policy comparison simulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygqZCq4IG2Ky"
      },
      "source": [
        "total_budget = 225.565034 # 225.56 M USD -> 576,000 M COP\n",
        "\n",
        "# Number of policies to compare\n",
        "policies_comparison = 0\n",
        "\n",
        "# Mean recovery time vectors\n",
        "fst_policy_time = []\n",
        "opt_policy_time = []\n",
        "chp_policy_time = []\n",
        "\n",
        "# Mean recovery cost vectors\n",
        "fst_policy_cost = []\n",
        "opt_policy_cost = []\n",
        "chp_policy_cost = []\n",
        "\n",
        "# Mean budget utilization vectors\n",
        "fst_policy_bud = []\n",
        "opt_policy_bud = []\n",
        "chp_policy_bud = []\n",
        "\n",
        "while policies_comparison <= 2:\n",
        "  # Number of runnings \n",
        "  n_of_interations = 0\n",
        "  while n_of_interations <= 100000 :\n",
        "  \n",
        "    recovery_cost = 0\n",
        "    budget_ut = 0\n",
        "    time = 0\n",
        "    state = 0\n",
        "    current_state_vector = [0]\n",
        "    next_state_vector = [0]\n",
        "\n",
        "    # Time vectors per policy\n",
        "    time_vector_fst = [0]\n",
        "    time_vector_opt = [0]\n",
        "    time_vector_chp = [0]\n",
        "\n",
        "    # Costs vectors per policy\n",
        "    cost_vector_fst = [0]\n",
        "    cost_vector_opt = [0]\n",
        "    cost_vector_chp = [0]\n",
        "\n",
        "    # Budget utilization vectors per policy\n",
        "    budget_fst = [0]\n",
        "    budget_opt = [0]\n",
        "    budget_chp = [0]\n",
        "    \n",
        "    if policies_comparison == 0:\n",
        "      while  state < (len(optimal_policy)-10):\n",
        "        # state transition\n",
        "        decision = 0 # fastest policy   \n",
        "        next_state = state_generator(exp_rates[decision],state)\n",
        "        \n",
        "        current_state_vector.append(state)\n",
        "        next_state_vector.append(next_state)\n",
        "      \n",
        "        # cumulative time (t + ∆t) // cumulative costs (t + ∆t) // cumulative budget utilization\n",
        "        time = time + np.random.exponential(exp_rates[decision][next_state].loc[state])\n",
        "        recovery_cost = recovery_cost + (fixed_cost_matrix[decision].loc[state] + time*cost_rate.loc[state])[0].astype(type('float', (float,), {})) # (1) [0] pandas core series to return numerical value (2) '.astype(type('float', (float,), {}))' to convert numpy.float64 into float\n",
        "        budget_ut = budget_ut + fixed_cost_matrix[decision].loc[state].astype(type('float', (float,), {}))\n",
        "\n",
        "        # Times and costs vector update\n",
        "        time_vector_fst.append(time)\n",
        "        cost_vector_fst.append(recovery_cost)\n",
        "        budget_fst.append(budget_ut/total_budget)\n",
        "\n",
        "        # State update\n",
        "        state = next_state\n",
        "      \n",
        "      # recording time and cost values per running\n",
        "      fst_policy_time.append(time_vector_fst[len(time_vector_fst)-1])\n",
        "      fst_policy_cost.append(cost_vector_fst[len(cost_vector_fst)-1])\n",
        "      fst_policy_bud.append(budget_fst[len(budget_fst)-1])\n",
        "\n",
        "    if policies_comparison == 1:\n",
        "      while  state < (len(optimal_policy)-10):\n",
        "        # state transition\n",
        "        decision = optimal_policy[state].astype(int) # optimal policy   \n",
        "        next_state = state_generator(exp_rates[decision],state)\n",
        "        \n",
        "        current_state_vector.append(state)\n",
        "        next_state_vector.append(next_state)\n",
        "      \n",
        "        # cumulative time (t + ∆t) // cumulative costs (t + ∆t) // cumulative budget utilization\n",
        "        time = time + np.random.exponential(exp_rates[decision][next_state].loc[state])\n",
        "        recovery_cost = recovery_cost + (fixed_cost_matrix[decision].loc[state] + time*cost_rate.loc[state])[0].astype(type('float', (float,), {})) # (1) [0] pandas core series to return numerical value (2) '.astype(type('float', (float,), {}))' to convert numpy.float64 into float\n",
        "        budget_ut = budget_ut + fixed_cost_matrix[decision].loc[state].astype(type('float', (float,), {}))\n",
        "\n",
        "        # Times and costs vector update\n",
        "        time_vector_opt.append(time) # optimal policy\n",
        "        cost_vector_opt.append(recovery_cost)\n",
        "        budget_opt.append(budget_ut/total_budget)\n",
        "\n",
        "        # State update\n",
        "        state = next_state\n",
        "      \n",
        "      # recording time and cost values per running\n",
        "      opt_policy_time.append(time_vector_opt[len(time_vector_opt)-1])\n",
        "      opt_policy_cost.append(cost_vector_opt[len(cost_vector_opt)-1])\n",
        "      opt_policy_bud.append(budget_opt[len(budget_opt)-1])\n",
        "\n",
        "    if policies_comparison == 2:\n",
        "      while  state < (len(optimal_policy)-10):\n",
        "        # state transition\n",
        "        decision = 4 # lowest cost policy\n",
        "        next_state = state_generator(exp_rates[decision],state)\n",
        "        \n",
        "        current_state_vector.append(state)\n",
        "        next_state_vector.append(next_state)\n",
        "      \n",
        "        # cumulative time (t + ∆t) // cumulative costs (t + ∆t) // cumulative budget utilization\n",
        "        time = time + np.random.exponential(exp_rates[decision][next_state].loc[state])\n",
        "        recovery_cost = recovery_cost + (fixed_cost_matrix[decision].loc[state] + time*cost_rate.loc[state])[0].astype(type('float', (float,), {})) # (1) [0] pandas core series to return numerical value (2) '.astype(type('float', (float,), {}))' to convert numpy.float64 into float\n",
        "        budget_ut = budget_ut + fixed_cost_matrix[decision].loc[state].astype(type('float', (float,), {}))\n",
        "\n",
        "\n",
        "        # Times and costs vector update\n",
        "        time_vector_chp.append(time)\n",
        "        cost_vector_chp.append(recovery_cost)\n",
        "        budget_chp.append(budget_ut/total_budget)\n",
        "\n",
        "        # State update\n",
        "        state = next_state\n",
        "      \n",
        "      # recording time and cost values per running\n",
        "      chp_policy_time.append(time_vector_chp[len(time_vector_chp)-1])\n",
        "      chp_policy_cost.append(cost_vector_chp[len(cost_vector_chp)-1])\n",
        "      chp_policy_bud.append(budget_chp[len(budget_chp)-1])\n",
        "\n",
        "    #updating running\n",
        "    n_of_interations = n_of_interations + 1\n",
        "  policies_comparison = policies_comparison + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c09y4b2OsMgZ"
      },
      "source": [
        "## Resilience assessment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZeKWencuARB"
      },
      "source": [
        "# Zobel metric\n",
        "def res_zobel(time, control = 120, X = .9):\n",
        "  T = control # Control time T = 120 Months\n",
        "  X = .9 # Performance loss 100%\n",
        "  resilience = 1 - ((X*time)/(2*T))\n",
        "  return resilience"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zobel metric\n",
        "def res_zobel(time, control = 120, X = .9):\n",
        "  resilience = 1 - ((X*time)/(2*control))\n",
        "  return resilience"
      ],
      "metadata": {
        "id": "pzGBZtLwpGO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkxnAVMqCVa3"
      },
      "source": [
        "## Confidence intervals function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHKmiH8XCZzg"
      },
      "source": [
        "def CI(dataset, cl):\n",
        "  LB = np.mean(dataset) - sps.norm.ppf(1-(1-cl)/2)*np.sqrt(np.var(dataset)/len(dataset))\n",
        "  UB = np.mean(dataset) + sps.norm.ppf(1-(1-cl)/2)*np.sqrt(np.var(dataset)/len(dataset))\n",
        "  return (LB,UB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDpSnZsNTTXf"
      },
      "source": [
        "## Simulation summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80qRgqJSTWSb"
      },
      "source": [
        "# Summary cols\n",
        "\n",
        "policies = ['Fastest policy', 'Optimal policy', 'Lowest-cost policy'] # policies vector\n",
        "mean_time = [np.mean(fst_policy_time), np.mean(opt_policy_time), np.mean(chp_policy_time)]\n",
        "mean_cost = [np.mean(fst_policy_cost), np.mean(opt_policy_cost), np.mean(chp_policy_cost)]\n",
        "mean_budget= [np.mean(fst_policy_bud), np.mean(opt_policy_bud), np.mean(chp_policy_bud)]\n",
        "std_cost = [np.std(fst_policy_cost), np.std(opt_policy_cost), np.std(chp_policy_cost)]\n",
        "std_time = [np.std(fst_policy_time), np.std(opt_policy_time), np.std(chp_policy_time)]\n",
        "resilience = [res_zobel(np.mean(fst_policy_time)), res_zobel(np.mean(opt_policy_time)), res_zobel(np.mean(chp_policy_time))]\n",
        "time_LB = [CI(fst_policy_time, .95)[0], CI(opt_policy_time, .95)[0], CI(chp_policy_time, .95)[0]]\n",
        "time_UB = [CI(fst_policy_time, .95)[1], CI(opt_policy_time, .95)[1], CI(chp_policy_time, .95)[1]]\n",
        "cost_LB = [CI(fst_policy_cost, .95)[0], CI(opt_policy_cost, .95)[0], CI(chp_policy_cost, .95)[0]]\n",
        "cost_UB = [CI(fst_policy_cost, .95)[1], CI(opt_policy_cost, .95)[1], CI(chp_policy_cost, .95)[1]]\n",
        "\n",
        "\n",
        "sim_summary = pd.DataFrame({'Polcies': policies, 'Mean recovery time': mean_time, 'Time lower-bound':time_LB, 'Time Upper-bound': time_UB,\n",
        "                                                 'Mean recovery cost': mean_cost, 'Cost lower-bound':cost_LB, 'Cost Upper-bound': cost_UB,\n",
        "                                                 'Mean budget utilizacion (%)': mean_budget,\n",
        "                                                 'Average resilience T* = 120 (Zobel)': resilience})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}